{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to C-ML","text":"<p>C-ML is a lightweight and modular machine learning library written in C. It is designed for educational purposes and lightweight applications, providing essential components for building and training neural networks.</p>"},{"location":"#why-choose-c-ml","title":"Why Choose C-ML?","text":"<ul> <li>Lightweight: Minimal dependencies and optimized for performance.</li> <li>Modular: Use only the components you need.</li> <li>Educational: Learn the fundamentals of machine learning by exploring the source code.</li> </ul>"},{"location":"#get-started","title":"Get Started","text":"<ul> <li>Installation Guide</li> <li>Features Overview</li> <li>Usage Examples</li> </ul>"},{"location":"#contribute","title":"Contribute","text":"<p>We welcome contributions! Check out the Contributing Guidelines to get started.</p> <p>Tip: Use the navigation bar on the left to explore the documentation.</p>"},{"location":"DesignGuide/","title":"C-ML Codebase Design Guide","text":"<p>This document provides the conventions and best practices for contributing to the C-ML codebase. Adhering to these guidelines will ensure consistency, readability, and maintainability throughout the project.</p>"},{"location":"DesignGuide/#1-general-coding-conventions","title":"1. General Coding Conventions","text":""},{"location":"DesignGuide/#11-indentation-and-formatting","title":"1.1 Indentation and Formatting","text":"<ul> <li>Indentation: Use 4 spaces for indentation (do not use tabs).</li> <li>Braces: Place opening braces <code>{</code> on the same line as the control statement (e.g., <code>if</code>, <code>for</code>, <code>while</code>).</li> </ul> <p>Example:   <pre><code>if (condition) {\n    // Do something\n}\n</code></pre></p> <ul> <li>Blank Lines: Use a single blank line to separate logical blocks of code. Avoid excessive blank lines.</li> </ul>"},{"location":"DesignGuide/#12-comments","title":"1.2 Comments","text":"<ul> <li>Single-line comments: Use <code>//</code> for brief explanations.</li> <li>Multi-line comments: Use <code>/* */</code> for longer descriptions.</li> <li>Doxygen-style comments: For documenting functions, structures, and files, use Doxygen-style comments (<code>/** */</code>).</li> </ul> <p>Example:   <pre><code>/**\n * @brief Calculates the forward pass of the pooling layer.\n * @param layer Pointer to the pooling layer structure.\n * @param input Pointer to the input data.\n * @param output Pointer to the output data.\n * @param input_size Size of the input data.\n * @return The number of output elements, or a negative error code.\n */\nint pooling_layer_forward(PoolingLayer *layer, const float *input, float *output, int input_size);\n</code></pre></p> <ul> <li>Avoid over-commenting: Do not comment obvious code or trivial operations. Comments should focus on explaining \"why\" something is done, not \"what\" is done.</li> </ul>"},{"location":"DesignGuide/#2-variable-naming-conventions","title":"2. Variable Naming Conventions","text":""},{"location":"DesignGuide/#21-general-naming-rules","title":"2.1 General Naming Rules","text":"<ul> <li>Variable names: Use snake_case for variables (e.g., <code>kernel_size</code>, <code>input_data</code>).</li> <li>Descriptive names: Choose meaningful names that indicate the purpose of the variable.</li> <li>Short names: Avoid one-character variable names except for loop counters (e.g., <code>i</code>, <code>j</code>).</li> </ul>"},{"location":"DesignGuide/#22-constants","title":"2.2 Constants","text":"<ul> <li>Constant names: Use UPPERCASE_SNAKE_CASE for constants (e.g., <code>MAX_BUFFER_SIZE</code>, <code>PI</code>).</li> </ul>"},{"location":"DesignGuide/#23-pointers","title":"2.3 Pointers","text":"<ul> <li>Pointer variables: Prefix pointer variables with <code>p_</code> to indicate they are pointers (e.g., <code>p_layer</code>, <code>p_input</code>).</li> </ul>"},{"location":"DesignGuide/#3-function-naming-conventions","title":"3. Function Naming Conventions","text":""},{"location":"DesignGuide/#31-general-rules","title":"3.1 General Rules","text":"<ul> <li>Function names: Use snake_case for function names (e.g., <code>initialize_layer</code>, <code>maxpooling_forward</code>).</li> <li>Function naming format: The function name should describe its purpose. Use the format:   <pre><code>&lt;module&gt;_&lt;action&gt;_&lt;specifics&gt;\n</code></pre>   Example: <code>pooling_layer_forward</code>, <code>activation_function_apply</code>.</li> </ul>"},{"location":"DesignGuide/#32-return-values","title":"3.2 Return Values","text":"<ul> <li>Success: Return <code>0</code> to indicate success.</li> <li>Errors: Use negative values for error codes (e.g., <code>-1</code>, <code>-2</code>), and define them in an enumeration.</li> </ul>"},{"location":"DesignGuide/#33-documentation","title":"3.3 Documentation","text":"<ul> <li>Document each function using Doxygen-style comments. Describe the function's purpose, parameters, and return values.</li> </ul>"},{"location":"DesignGuide/#4-layer-design-conventions","title":"4. Layer Design Conventions","text":""},{"location":"DesignGuide/#41-structure-naming","title":"4.1 Structure Naming","text":"<ul> <li>Structure names: Use PascalCase for structure names (e.g., <code>PoolingLayer</code>, <code>MaxPoolingLayer</code>).</li> </ul>"},{"location":"DesignGuide/#42-structure-fields","title":"4.2 Structure Fields","text":"<ul> <li>Structure field names: Use snake_case for structure fields (e.g., <code>kernel_size</code>, <code>stride</code>).</li> </ul>"},{"location":"DesignGuide/#43-layer-functions","title":"4.3 Layer Functions","text":"<ul> <li>Each layer should have the following functions:</li> <li><code>layer_create</code>: Allocates and initializes the layer.</li> <li><code>layer_forward</code>: Executes the forward pass.</li> <li><code>layer_output_size</code>: Computes the output size.</li> <li><code>layer_free</code>: Frees the allocated memory.</li> </ul> <p>Example: <pre><code>PoolingLayer *pooling_layer_create(int kernel_size, int stride);\nint pooling_layer_forward(PoolingLayer *layer, const float *input, float *output, int input_size);\nint pooling_layer_output_size(int input_size, int kernel_size, int stride);\nvoid pooling_layer_free(PoolingLayer *layer);\n</code></pre></p>"},{"location":"DesignGuide/#5-debugging-conventions","title":"5. Debugging Conventions","text":""},{"location":"DesignGuide/#51-debug-logging","title":"5.1 Debug Logging","text":"<ul> <li> <p>Enable/disable logs: Use the <code>set_log_level(LOG_LEVEL_*)</code> macro to configure the global log level.   <pre><code>#include \"include/logging.h\"\n\nset_log_level(LOG_LEVEL_DEBUG);\nset_log_level(LOG_LEVEL_INFO);\nset_log_level(LOG_LEVEL_WARNING);\nset_log_level(LOG_LEVEL_ERROR);\n</code></pre></p> </li> <li> <p>Log appropriately: Use the <code>#LOG_*</code> macros to conditionally log messages.   <pre><code>LOG_DEBUG(\"%s is a debug message.\", message);\nLOG_INFO(\"Count is %d.\", count);\nLOG_WARNING(\"Tensor Bloat is %d unreasonable.\", bloat_factor);\nLOG_ERROR(\"NeuralNetwork is NULL.\");\n</code></pre></p> </li> </ul>"},{"location":"DesignGuide/#52-error-messages","title":"5.2 Error Messages","text":"<ul> <li>Error messages: Use the <code>LOG_ERROR</code> macro. Include relevant parameter values.   <pre><code>LOG_ERROR(\"Invalid parameter (%d).\", param);\n</code></pre></li> </ul>"},{"location":"DesignGuide/#53-log-message-formatting","title":"5.3 Log Message Formatting","text":"<ul> <li>Log messages are automatically formatted for you when you use the <code>LOG_*</code> macros. <pre><code>Compiling and running test_logging...\nRunning logging tests...\n2025-04-07 03:37:33 [DEBUG] test/Core/test_logging.c:18 main(): This is a debug message: 42\n2025-04-07 03:37:33 [INFO] test/Core/test_logging.c:19 main(): This is an info message: hello\n2025-04-07 03:37:33 [WARNING] test/Core/test_logging.c:20 main(): This is a warning message: 3.14\n2025-04-07 03:37:33 [ERROR] test/Core/test_logging.c:21 main(): This is an error message: X\n</code></pre></li> </ul>"},{"location":"DesignGuide/#6-file-organization","title":"6. File Organization","text":""},{"location":"DesignGuide/#61-directory-structure","title":"6.1 Directory Structure","text":"<ul> <li>Source files: Place <code>.c</code> files in the <code>src/</code> directory.</li> <li>Header files: Place <code>.h</code> files in the <code>include/</code> directory.</li> <li>Subdirectories: Use subdirectories for logical groupings, e.g., <code>Layers/</code> for layers, <code>core/</code> for core functionalities.</li> </ul>"},{"location":"DesignGuide/#62-file-naming","title":"6.2 File Naming","text":"<ul> <li>File names: Use snake_case for file names (e.g., <code>pooling.c</code>, <code>maxpooling.h</code>).</li> </ul>"},{"location":"DesignGuide/#7-error-handling","title":"7. Error Handling","text":""},{"location":"DesignGuide/#71-error-codes","title":"7.1 Error Codes","text":"<ul> <li>Error codes: Define error codes using an enum in <code>src/core/error_codes.h</code>.   <pre><code>typedef enum {\n    CM_SUCCESS = 0,\n    CM_NULL_POINTER_ERROR = -1,\n    CM_MEMORY_ALLOCATION_ERROR = -2,\n    // Add other error codes as needed\n} CM_Error;\n</code></pre></li> </ul>"},{"location":"DesignGuide/#72-error-propagation","title":"7.2 Error Propagation","text":"<ul> <li>Propagate errors: Functions should return error codes (<code>CM_Error</code>), which can be checked by the caller.</li> </ul>"},{"location":"DesignGuide/#8-testing","title":"8. Testing","text":""},{"location":"DesignGuide/#81-unit-tests","title":"8.1 Unit Tests","text":"<ul> <li>Test directory: Place unit test files in the <code>tests/</code> directory.</li> <li>Test coverage: Ensure all layers and functions are tested.</li> </ul>"},{"location":"DesignGuide/#82-test-naming","title":"8.2 Test Naming","text":"<ul> <li>File naming: Use the format <code>&lt;module&gt;_test.c</code> for test files (e.g., <code>pooling_test.c</code>).</li> </ul>"},{"location":"DesignGuide/#9-code-review-checklist","title":"9. Code Review Checklist","text":"<p>Before submitting a pull request: 1. Code follows the naming and formatting conventions. 2. Functions are properly documented with Doxygen-style comments. 3. Logs use <code>LOG_DEBUG</code>, <code>LOG_INFO</code>, <code>LOG_WARNING</code> or <code>LOG_ERROR</code> appropriately. 4. Error handling is implemented and tested. 5. Unit tests are written and pass successfully.</p>"},{"location":"DesignGuide/#10-example-code","title":"10. Example Code","text":""},{"location":"DesignGuide/#101-pooling-layer-example","title":"10.1 Pooling Layer Example","text":"<pre><code>#include \"pooling.h\"\n\nint main() {\n    // Create pooling layer\n    PoolingLayer *layer = pooling_layer_create(2, 2);\n    if (!layer) {\n        LOG_ERROR(\"Failed to create pooling layer.\\n\");\n        return CM_NULL_POINTER_ERROR;\n    }\n\n    // Perform forward pass\n    float input[] = {1.0, 2.0, 3.0, 4.0};\n    float output[2];\n    int output_size = pooling_layer_forward(layer, input, output, 4);\n\n    if (output_size &lt; 0) {\n        LOG_ERROR(\"Error during forward pass.\\n\");\n        pooling_layer_free(layer);\n        return output_size;\n    }\n\n    // Print output\n    for (int i = 0; i &lt; output_size; i++) {\n        LOG_INFO(\"$2\");Output[%d]: %f\\n\", i, output[i]);\n    }\n\n    // Free resources\n    pooling_layer_free(layer);\n    return CM_SUCCESS;\n}\n</code></pre>"},{"location":"DesignGuide/#11-memory-management","title":"11. Memory Management","text":""},{"location":"DesignGuide/#111-custom-memory-allocation","title":"11.1 Custom Memory Allocation","text":"<ul> <li>Use custom memory management functions:   <pre><code>void *cm_safe_malloc(size_t size, const char *file, int line);\nvoid cm_safe_free(void *ptr);\n</code></pre></li> </ul>"},{"location":"DesignGuide/#112-memory-usage","title":"11.2 Memory Usage","text":"<ul> <li>Always use <code>cm_safe_malloc</code> for memory allocation and <code>cm_safe_free</code> for deallocation.</li> <li>The <code>cm_safe_malloc</code> function should include file and line numbers for easier debugging.</li> </ul>"},{"location":"DesignGuide/#12-import-minimization","title":"12. Import Minimization","text":""},{"location":"DesignGuide/#121-avoid-unnecessary-imports","title":"12.1 Avoid Unnecessary Imports","text":"<ul> <li>Include only the necessary header files. Remove any unused imports to reduce compilation time and improve code clarity.</li> </ul>"},{"location":"DesignGuide/#122-minimize-library-imports","title":"12.2 Minimize Library Imports","text":"<ul> <li>When using external libraries, only import the specific components needed rather than the entire library. This reduces the size of the compiled code.</li> </ul>"},{"location":"DesignGuide/#13-comment-placement-guidelines","title":"13. Comment Placement Guidelines","text":""},{"location":"DesignGuide/#131-implementation-comments-in-c-files","title":"13.1 Implementation Comments in <code>.c</code> Files","text":"<ul> <li>Use detailed comments in <code>.c</code> files to describe the implementation logic.</li> <li>Include explanations for complex computations, algorithms, or formulas.</li> <li>Focus on \"how\" the function works.</li> </ul>"},{"location":"DesignGuide/#132-interface-comments-in-h-files","title":"13.2 Interface Comments in <code>.h</code> Files","text":"<ul> <li>Use concise comments in <code>.h</code> files to describe the function's purpose, parameters, and return values.</li> <li>Focus on \"what\" the function does and how other code will interact with it.</li> <li>Avoid including implementation details in <code>.h</code> files.</li> </ul> <p>Example: <pre><code>// .h file\n\n/**\n * @brief Applies the GELU activation function.\n * @param input Pointer to the input array.\n * @param output Pointer to the output array.\n * @param size Number of elements in the input array.\n * @return CM_SUCCESS on success, or an error code on failure.\n */\nint gelu_activation(const float *input, float *output, int size);\n</code></pre></p> <pre><code>// .c file\n\n/**\n * @brief Applies the GELU activation function.\n *\n * The GELU function is defined as:\n * GELU(x) = 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))\n *\n * This implementation uses an approximation for efficiency.\n *\n * @param input Pointer to the input array.\n * @param output Pointer to the output array.\n * @param size Number of elements in the input array.\n * @return CM_SUCCESS on success, or an error code on failure.\n */\n\nint gelu_activation(const float *input, float *output, int size) {\n    for (int i = 0; i &lt; size; i++) {\n        float x = input[i];\n        output[i] = 0.5 * x * (1 + tanh(sqrt(2 / M_PI) * (x + 0.044715 * pow(x, 3))));\n    }\n    return CM_SUCCESS;\n}\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome! Please follow these steps:</p> <ol> <li>Fork the repository.</li> <li>Create a new branch for your feature or bug fix.</li> <li>Commit your changes with clear messages.</li> <li>Submit a pull request.</li> </ol>"},{"location":"development_guidelines/","title":"Development Guidelines","text":""},{"location":"development_guidelines/#code-style","title":"Code Style","text":"<ul> <li>Follow consistent indentation and naming conventions.</li> <li>Use meaningful variable names.</li> <li>Add comments to explain complex logic.</li> </ul>"},{"location":"development_guidelines/#error-handling","title":"Error Handling","text":"<ul> <li>Check for null pointers and invalid inputs.</li> <li>Use <code>LOG_ERROR</code> macro to log errors</li> <li>return a <code>CM_Error</code> status code if recoverable...</li> <li>or exit gracefully if not.</li> </ul>"},{"location":"development_guidelines/#memory-management","title":"Memory Management","text":"<ul> <li>Allocate memory dynamically where necessary.</li> <li>Free allocated memory to prevent leaks.</li> </ul>"},{"location":"development_guidelines/#testing","title":"Testing","text":"<ul> <li>Write unit tests for all functions.</li> <li>Use assertions to validate expected behavior.</li> </ul>"},{"location":"features/","title":"Features","text":"<ul> <li>\ud83e\udde9 Layers: Dense, Dropout, Flatten, Pooling, Max-Pooling</li> <li>\u26a1 Activations: ReLU, Sigmoid, Tanh, Softmax, ELU, Leaky ReLU, Linear, GeLU</li> <li>\ud83d\udcc9 Loss Functions: Mean Squared Error, Binary Cross-Entropy, Focal Loss, etc.</li> <li>\ud83d\ude80 Optimizers: SGD, Adam, RMSprop</li> <li>\ud83d\udd27 Preprocessing: Label Encoding, One-Hot Encoding, Standard Scaler, Min-Max Scaler</li> <li>\ud83d\udee1\ufe0f Regularizers: L1, L2, Combined L1-L2</li> <li>\u2705 Test Coverage: Comprehensive unit tests for all modules.</li> </ul>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>GCC (GNU Compiler Collection)</li> <li><code>make</code> build tool</li> <li>Supported Platforms: Linux, macOS, Windows (via WSL)</li> </ul>"},{"location":"installation/#steps","title":"Steps","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/jaywyawhare/C-ML.git\ncd C-ML\n</code></pre></p> </li> <li> <p>Build the project:    <pre><code>make\n</code></pre></p> </li> <li> <p>Run the example program:    <pre><code>./bin/main\n</code></pre></p> </li> <li> <p>Run all tests:    <pre><code>make test\n</code></pre></p> </li> </ol> <p>Run specific tests:    <pre><code>make test TEST_SRCS=\"test/Layers/test_dense.c\"\n</code></pre></p> <p>Note: Ensure that GCC and <code>make</code> are installed and available in your system's PATH.</p>"},{"location":"license/","title":"License","text":"<p>This project is licensed under the DBaJ-NC-CFL License. See the LICENSE file for details.</p>"},{"location":"testing/","title":"Testing","text":"<p>Run the tests using the <code>make test</code> command: <pre><code>make test\n</code></pre></p> <p>Each module has a corresponding test file in the <code>test/</code> directory. The tests validate the correctness of the implementation and ensure robustness.</p>"},{"location":"usage/","title":"Usage","text":"<p>This page provides an example of how to use the C-ML library to create and train a simple neural network.</p>"},{"location":"usage/#neural-network-training-example","title":"Neural Network Training Example","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include \"include/Core/training.h\"\n#include \"include/Core/dataset.h\"\n\nint main()\n{\n    NeuralNetwork *network = create_neural_network(2);\n    build_network(network, OPTIMIZER_ADAM, 0.1f, LOSS_MSE, 0.0f, 0.0f);\n    model_add(network, LAYER_DENSE, ACTIVATION_RELU, 2, 4, 0.0f, 0, 0);\n    model_add(network, LAYER_DENSE, ACTIVATION_TANH, 4, 4, 0.0f, 0, 0);\n    model_add(network, LAYER_DENSE, ACTIVATION_SIGMOID, 4, 1, 0.0f, 0, 0);\n\n    float X_data[4][2] = {{0.0f, 0.0f}, {0.0f, 1.0f}, {1.0f, 0.0f}, {1.0f, 1.0f}};\n    float y_data[4][1] = {{0.0f}, {1.0f}, {1.0f}, {1.0f}};\n\n    Dataset *dataset = dataset_create();\n    dataset_load_arrays(dataset, (float *)X_data, (float *)y_data, 4, 2, 1);\n\n    summary(network);\n\n    train_network(network, dataset, 30);\n    test_network(network, dataset-&gt;X, dataset-&gt;y, dataset-&gt;num_samples, NULL);\n\n    dataset_free(dataset);\n    free_neural_network(network);\n\n    return 0;\n}\n</code></pre> <p>This example demonstrates how to: - Create a neural network with three dense layers - Use different activation functions (ReLU, Tanh, Sigmoid) - Create and load a dataset - Train the network using the Adam optimizer - Test the network's performance</p>"},{"location":"modules/activations/","title":"Activations","text":""},{"location":"modules/activations/#relu","title":"ReLU","text":"<ul> <li>Description: Rectified Linear Unit activation function.</li> <li>Function: <code>relu(float x)</code></li> <li>File: <code>relu.c</code></li> </ul>"},{"location":"modules/activations/#sigmoid","title":"Sigmoid","text":"<ul> <li>Description: Sigmoid activation function.</li> <li>Function: <code>sigmoid(float x)</code></li> <li>File: <code>sigmoid.c</code></li> </ul>"},{"location":"modules/activations/#tanh","title":"Tanh","text":"<ul> <li>Description: Hyperbolic tangent activation function.</li> <li>Function: <code>tanH(float x)</code></li> <li>File: <code>tanh.c</code></li> </ul>"},{"location":"modules/activations/#softmax","title":"Softmax","text":"<ul> <li>Description: Converts logits into probabilities.</li> <li>Function: <code>softmax(float *z, int n)</code></li> <li>File: <code>softmax.c</code></li> </ul>"},{"location":"modules/activations/#elu","title":"ELU","text":"<ul> <li>Description: Exponential Linear Unit activation function.</li> <li>Function: <code>elu(float x, float alpha)</code></li> <li>File: <code>elu.c</code></li> </ul>"},{"location":"modules/activations/#leaky-relu","title":"Leaky ReLU","text":"<ul> <li>Description: Leaky version of ReLU to allow small gradients for negative inputs.</li> <li>Function: <code>leaky_relu(float x)</code></li> <li>File: <code>leaky_relu.c</code></li> </ul>"},{"location":"modules/activations/#linear","title":"Linear","text":"<ul> <li>Description: Linear activation function (identity function).</li> <li>Function: <code>linear(float x)</code></li> <li>File: <code>linear.c</code></li> </ul>"},{"location":"modules/activations/#gelu","title":"GELU","text":"<ul> <li>Description: Gaussian Error Linear Unit activation function.</li> <li>Function: <code>gelu(float x)</code></li> <li>File: <code>gelu.c</code></li> </ul>"},{"location":"modules/core/","title":"Core Modules","text":"<p>This section documents the core modules of the C-ML library, which provide essential functionalities for building and training neural networks.</p>"},{"location":"modules/core/#training-module","title":"Training Module","text":"<ul> <li>Description: Provides functions for creating, building, training, and evaluating neural networks.</li> <li>Functions:<ul> <li><code>NeuralNetwork *create_neural_network(int input_size)</code>: Creates a new neural network.</li> <li><code>CM_Error build_network(NeuralNetwork *network, OptimizerType optimizer_type, float learning_rate, int loss_function, float l1_lambda, float l2_lambda)</code>: Builds the neural network by setting the optimizer, loss function, and regularization parameters.</li> <li><code>CM_Error add_layer(NeuralNetwork *network, LayerConfig config)</code>: Adds a layer to the neural network.</li> <li><code>CM_Error model_add(NeuralNetwork *network, LayerType type, ActivationType activation, int input_size, int output_size, float rate, int kernel_size, int stride)</code>: Adds a layer to the neural network using a simplified interface.</li> <li><code>CM_Error forward_pass(NeuralNetwork *network, float *input, float *output, int input_size, int output_size, int is_training)</code>: Performs a forward pass through the network.</li> <li><code>float calculate_loss(float *predicted, float *actual, int size, LossType loss_type)</code>: Calculates the loss between predicted and actual values.</li> <li><code>void calculate_loss_gradient(float *predicted, float *actual, float *gradient, int size, LossType loss_type)</code>: Calculates the gradient of the loss function.</li> <li><code>CM_Error train_network(NeuralNetwork *network, float **X_train, float **y_train, int num_samples, int input_size, int output_size, int batch_size, int epochs)</code>: Trains the neural network.</li> <li><code>CM_Error evaluate_network(NeuralNetwork *network, float **X_test, float **y_test, int num_samples, int input_size, int output_size, int *metrics, int num_metrics, float *results)</code>: Evaluates the neural network on a given dataset.</li> <li><code>CM_Error test_network(NeuralNetwork *network, float **X_test, float **y_test, int num_samples, int input_size, int output_size, int *metrics, int num_metrics, float *results)</code>: Tests the neural network (alias for evaluate_network).</li> <li><code>CM_Error free_neural_network(NeuralNetwork *network)</code>: Frees memory allocated for the neural network.</li> <li><code>void summary(NeuralNetwork *network)</code>: Prints a summary of the neural network architecture.</li> </ul> </li> <li>File: <code>training.c</code></li> </ul>"},{"location":"modules/core/#memory-management-module","title":"Memory Management Module","text":"<ul> <li>Description: Provides safe memory allocation and deallocation functions.</li> <li>Functions:<ul> <li><code>void *cm_safe_malloc(size_t size, const char *file, int line)</code>: Allocates memory safely and logs the file and line number in case of failure.</li> <li><code>void cm_safe_free(void **ptr)</code>: Frees allocated memory safely and sets the pointer to NULL.</li> </ul> </li> <li>File: <code>memory_management.c</code></li> </ul>"},{"location":"modules/layers/","title":"Layers","text":""},{"location":"modules/layers/#dense-layer","title":"Dense Layer","text":"<ul> <li>Description: Fully connected layer where each input is connected to each output.</li> <li>Function: <ul> <li><code>initialize_dense(DenseLayer *layer, int input_size, int output_size)</code></li> <li><code>forward_dense(DenseLayer *layer, float *input, float *output)</code></li> <li><code>backward_dense(DenseLayer *layer, float *input, float *output, float *d_output, float *d_input, float *d_weights, float *d_biases)</code></li> <li><code>update_dense(DenseLayer *layer, float *d_weights, float *d_biases, float learning_rate)</code></li> <li><code>free_dense(DenseLayer *layer)</code></li> </ul> </li> <li>File: <code>dense.c</code></li> </ul>"},{"location":"modules/layers/#dropout-layer","title":"Dropout Layer","text":"<ul> <li>Description: Randomly sets a fraction of input units to zero during training to prevent overfitting.</li> <li>Function: <ul> <li><code>initialize_dropout(DropoutLayer *layer, float dropout_rate)</code></li> <li><code>forward_dropout(DropoutLayer *layer, float *input, float *output, int size)</code></li> <li><code>backward_dropout(DropoutLayer *layer, float *input, float *output, float *d_output, float *d_input, int size)</code></li> </ul> </li> <li>File: <code>dropout.c</code></li> </ul>"},{"location":"modules/layers/#flatten-layer","title":"Flatten Layer","text":"<ul> <li>Description: Flattens the input without affecting the batch size.</li> <li>Function: <ul> <li><code>initializeFlatten(FlattenLayer *layer, int input_size)</code></li> <li><code>forwardFlatten(FlattenLayer *layer, float *input, float *output)</code></li> <li><code>backwardFlatten(FlattenLayer *layer, float *input, float *output, float *d_output, float *d_input)</code></li> <li><code>freeFlatten(FlattenLayer *layer)</code></li> </ul> </li> <li>File: <code>flatten.c</code></li> </ul>"},{"location":"modules/layers/#pooling-layer","title":"Pooling Layer","text":"<ul> <li>Description: Reduces the spatial size of the input volume.</li> <li>Function: <ul> <li><code>initialize_pooling(PoolingLayer *layer, int kernel_size, int stride)</code></li> <li><code>compute_pooling_output_size(int input_size, int kernel_size, int stride)</code></li> <li><code>forward_pooling(PoolingLayer *layer, const float *input, float *output, int input_size)</code></li> <li><code>free_pooling(PoolingLayer *layer)</code></li> </ul> </li> <li>File: <code>pooling.c</code></li> </ul>"},{"location":"modules/layers/#max-pooling-layer","title":"Max-Pooling Layer","text":"<ul> <li>Description: Applies max pooling operation to the input.</li> <li>Function: <ul> <li><code>initialize_maxpooling(MaxPoolingLayer *layer, int kernel_size, int stride)</code></li> <li><code>compute_maxpooling_output_size(int input_size, int kernel_size, int stride)</code></li> <li><code>forward_maxpooling(MaxPoolingLayer *layer, const float *input, float *output, int input_size)</code></li> <li><code>free_maxpooling(MaxPoolingLayer *layer)</code></li> </ul> </li> <li>File: <code>maxpooling.c</code></li> </ul>"},{"location":"modules/loss_functions/","title":"Loss Functions","text":""},{"location":"modules/loss_functions/#mean-squared-error","title":"Mean Squared Error","text":"<ul> <li>Description: Measures the average squared difference between predictions and actual values.</li> <li>Function: <code>mean_squared_error(float *y, float *yHat, int n)</code></li> <li>File: <code>mean_squared_error.c</code></li> </ul>"},{"location":"modules/loss_functions/#binary-cross-entropy","title":"Binary Cross-Entropy","text":"<ul> <li>Description: Loss function for binary classification tasks.</li> <li>Function: <code>binary_cross_entropy_loss(float *yHat, float *y, int size)</code></li> <li>File: <code>binary_cross_entropy_loss.c</code></li> </ul>"},{"location":"modules/loss_functions/#focal-loss","title":"Focal Loss","text":"<ul> <li>Description: Focuses on hard-to-classify examples by down-weighting easy examples.</li> <li>Function: <code>focal_loss(float *y, float *yHat, int n, float gamma)</code></li> <li>File: <code>focal_loss.c</code></li> </ul>"},{"location":"modules/loss_functions/#mean-absolute-error","title":"Mean Absolute Error","text":"<ul> <li>Description: Measures the average absolute difference between predictions and actual values.</li> <li>Function: <code>mean_absolute_error(float *y, float *yHat, int n)</code></li> <li>File: <code>mean_absolute_error.c</code></li> </ul>"},{"location":"modules/loss_functions/#mean-absolute-percentage-error","title":"Mean Absolute Percentage Error","text":"<ul> <li>Description: Measures the percentage difference between predictions and actual values.</li> <li>Function: <code>mean_absolute_percentage_error(float *y, float *yHat, int n)</code></li> <li>File: <code>mean_absolute_percentage_error.c</code></li> </ul>"},{"location":"modules/loss_functions/#root-mean-squared-error","title":"Root Mean Squared Error","text":"<ul> <li>Description: Square root of the mean squared error.</li> <li>Function: <code>root_mean_squared_error(float *y, float *yHat, int n)</code></li> <li>File: <code>root_mean_squared_error.c</code></li> </ul>"},{"location":"modules/loss_functions/#reduce-mean","title":"Reduce Mean","text":"<ul> <li>Description: Computes the mean of an array of values.</li> <li>Function: <code>reduce_mean(float *loss, int size)</code></li> <li>File: <code>reduce_mean.c</code></li> </ul>"},{"location":"modules/loss_functions/#huber-loss","title":"Huber Loss","text":"<ul> <li>Description: Huber Loss function.</li> <li>Function: <code>huber_loss(float *y, float *yHat, int n)</code></li> <li>File: <code>huber_loss.c</code></li> </ul>"},{"location":"modules/loss_functions/#kld-loss","title":"KLD Loss","text":"<ul> <li>Description: Kullback-Leibler Divergence Loss function.</li> <li>Function: <code>kld_loss(float *p, float *q, int n)</code></li> <li>File: <code>kld_loss.c</code></li> </ul>"},{"location":"modules/loss_functions/#log-cosh-loss","title":"Log-Cosh Loss","text":"<ul> <li>Description: Log-Cosh Loss function.</li> <li>Function: <code>log_cosh_loss(float *y, float *yHat, int n)</code></li> <li>File: <code>log_cosh_loss.c</code></li> </ul>"},{"location":"modules/loss_functions/#poisson-loss","title":"Poisson Loss","text":"<ul> <li>Description: Poisson Loss function.</li> <li>Function: <code>poisson_loss(float *y, float *yHat, int n)</code></li> <li>File: <code>poisson_loss.c</code></li> </ul>"},{"location":"modules/loss_functions/#smooth-l1-loss","title":"Smooth L1 Loss","text":"<ul> <li>Description: Smooth L1 Loss function.</li> <li>Function: <code>smooth_l1_loss(float *y, float *yHat, int n)</code></li> <li>File: <code>smooth_l1_loss.c</code></li> </ul>"},{"location":"modules/loss_functions/#tversky-loss","title":"Tversky Loss","text":"<ul> <li>Description: Tversky Loss function.</li> <li>Function: <code>tversky_loss(float *y, float *yHat, int n)</code></li> <li>File: <code>tversky_loss.c</code></li> </ul>"},{"location":"modules/loss_functions/#cosine-similarity-loss","title":"Cosine Similarity Loss","text":"<ul> <li>Description: Cosine Similarity Loss function.</li> <li>Function: <code>cosine_similarity_loss(float *y, float *yHat, int n)</code></li> <li>File: <code>cosine_similarity_loss.c</code></li> </ul>"},{"location":"modules/metrics/","title":"Metrics","text":""},{"location":"modules/metrics/#accuracy","title":"Accuracy","text":"<ul> <li>Description: Calculates the accuracy of the model.</li> <li>Function: <code>accuracy(float *y, float *yHat, int n)</code></li> <li>File: <code>accuracy.c</code></li> </ul>"},{"location":"modules/metrics/#balanced-accuracy","title":"Balanced Accuracy","text":"<ul> <li>Description: Calculates the balanced accuracy of the model.</li> <li>Function: <code>balanced_accuracy(float *y, float *yHat, int n)</code></li> <li>File: <code>balanced_accuracy.c</code></li> </ul>"},{"location":"modules/metrics/#cohens-kappa","title":"Cohen's Kappa","text":"<ul> <li>Description: Calculates Cohen's Kappa coefficient.</li> <li>Function: <code>cohens_kappa(float *y, float *yHat, int n)</code></li> <li>File: <code>cohens_kappa.c</code></li> </ul>"},{"location":"modules/metrics/#f1-score","title":"F1 Score","text":"<ul> <li>Description: Calculates the F1 score of the model.</li> <li>Function: <code>f1_score(float *y, float *yHat, int n)</code></li> <li>File: <code>f1_score.c</code></li> </ul>"},{"location":"modules/metrics/#iou-intersection-over-union","title":"IOU (Intersection over Union)","text":"<ul> <li>Description: Calculates the Intersection over Union.</li> <li>Function: <code>iou(float *y, float *yHat, int n)</code></li> <li>File: <code>iou.c</code></li> </ul>"},{"location":"modules/metrics/#mcc-matthews-correlation-coefficient","title":"MCC (Matthews Correlation Coefficient)","text":"<ul> <li>Description: Calculates the Matthews Correlation Coefficient.</li> <li>Function: <code>mcc(float *y, float *yHat, int n)</code></li> <li>File: <code>mcc.c</code></li> </ul>"},{"location":"modules/metrics/#mean-absolute-error","title":"Mean Absolute Error","text":"<ul> <li>Description: Calculates the Mean Absolute Error.</li> <li>Function: <code>mean_absolute_error(float *y, float *yHat, int n)</code></li> <li>File: <code>mean_absolute_error.c</code></li> </ul>"},{"location":"modules/metrics/#mean-absolute-percentage-error","title":"Mean Absolute Percentage Error","text":"<ul> <li>Description: Calculates the Mean Absolute Percentage Error.</li> <li>Function: <code>mean_absolute_percentage_error(float *y, float *yHat, int n)</code></li> <li>File: <code>mean_absolute_percentage_error.c</code></li> </ul>"},{"location":"modules/metrics/#precision","title":"Precision","text":"<ul> <li>Description: Calculates the precision of the model.</li> <li>Function: <code>precision(float *y, float *yHat, int n)</code></li> <li>File: <code>precision.c</code></li> </ul>"},{"location":"modules/metrics/#r2-score","title":"R2 Score","text":"<ul> <li>Description: Calculates the R2 score (coefficient of determination).</li> <li>Function: <code>r2_score(float *y_true, float *y_pred, int size)</code></li> <li>File: <code>r2_score.c</code></li> </ul>"},{"location":"modules/metrics/#recall","title":"Recall","text":"<ul> <li>Description: Calculates the recall of the model.</li> <li>Function: <code>recall(float *y, float *yHat, int n)</code></li> <li>File: <code>recall.c</code></li> </ul>"},{"location":"modules/metrics/#root-mean-squared-error","title":"Root Mean Squared Error","text":"<ul> <li>Description: Calculates the Root Mean Squared Error.</li> <li>Function: <code>root_mean_squared_error(float *y, float *yHat, int n)</code></li> <li>File: <code>root_mean_squared_error.c</code></li> </ul>"},{"location":"modules/metrics/#specificity","title":"Specificity","text":"<ul> <li>Description: Calculates the Specificity.</li> <li>Function: <code>Specificity(float *y, float *yHat, int n)</code></li> <li>File: <code>Specificity.c</code></li> </ul>"},{"location":"modules/optimizers/","title":"Optimizers","text":""},{"location":"modules/optimizers/#sgd-stochastic-gradient-descent","title":"SGD (Stochastic Gradient Descent)","text":"<ul> <li>Description: Basic optimizer that updates weights using gradients.</li> <li>Function: <code>float sgd(float x, float y, float lr, float *w, float *b)</code></li> <li>File: <code>sgd.c</code></li> </ul>"},{"location":"modules/optimizers/#adam","title":"Adam","text":"<ul> <li>Description: Adaptive optimizer combining momentum and RMSprop.</li> <li>Function: <code>float adam(float x, float y, float lr, float *w, float *b, float *v_w, float *v_b, float *s_w, float *s_b, float beta1, float beta2, float epsilon)</code></li> <li>File: <code>Adam.c</code></li> </ul>"},{"location":"modules/optimizers/#rmsprop","title":"RMSprop","text":"<ul> <li>Description: Optimizer that scales learning rates based on recent gradients.</li> <li>Function: <code>float rms_prop(float x, float y, float lr, float *w, float *b, float *cache_w, float *cache_b, float epsilon, float beta)</code></li> <li>File: <code>RMSprop.c</code></li> </ul>"},{"location":"modules/preprocessing/","title":"Preprocessing","text":""},{"location":"modules/preprocessing/#standard-scaler","title":"Standard Scaler","text":"<ul> <li>Description: Scales data to have zero mean and unit variance.</li> <li>Functions:</li> <li><code>float *standard_scaler(float *x, int size)</code></li> <li>File: <code>standard_scaler.c</code></li> </ul>"},{"location":"modules/preprocessing/#min-max-scaler","title":"Min-Max Scaler","text":"<ul> <li>Description: Scales data to a specified range (default: [0, 1]).</li> <li>Functions:</li> <li><code>float *min_max_scaler(float *x, int size)</code></li> <li>File: <code>min_max_scaler.c</code></li> </ul>"},{"location":"modules/preprocessing/#label-encoder","title":"Label Encoder","text":"<ul> <li>Description: Encodes categorical labels as integers.</li> <li>Functions:</li> <li><code>int *label_encoder(char *x, int size, CharMap **map, int *mapSize)</code></li> <li><code>char *label_decoder(int *x, int size, CharMap *map, int mapSize)</code></li> <li><code>void free_label_memory(CharMap *map, int *encoded, char *decoded)</code></li> <li>File: <code>label_encoder.c</code></li> </ul>"},{"location":"modules/preprocessing/#one-hot-encoder","title":"One-Hot Encoder","text":"<ul> <li>Description: Encodes categorical labels as one-hot vectors.</li> <li>Functions:</li> <li><code>int *one_hot_encoding(char *x, int size, CharMap **map, int *mapSize)</code></li> <li><code>char *one_hot_decoding(int *x, int size, CharMap *map, int mapSize)</code></li> <li><code>void free_one_hot_memory(int *x, char *y, CharMap *map)</code></li> <li>File: <code>one_hot_encoder.c</code></li> </ul>"},{"location":"modules/regularizers/","title":"Regularizers","text":""},{"location":"modules/regularizers/#l1-regularization","title":"L1 Regularization","text":"<ul> <li>Description: Adds the absolute value of weights to the loss function to encourage sparsity.</li> <li>Function: <code>float l1(float x, float y, float lr, float *w, float *b, float *v_w, float *v_b, float *s_w, float *s_b, float beta1, float beta2, float epsilon)</code></li> <li>File: <code>l1.c</code></li> </ul>"},{"location":"modules/regularizers/#l2-regularization","title":"L2 Regularization","text":"<ul> <li>Description: Adds the squared value of weights to the loss function to prevent overfitting.</li> <li>Function: <code>float l2(float x, float y, float lr, float *w, float *b, float *v_w, float *v_b, float *s_w, float *s_b, float beta1, float beta2, float epsilon, float reg_l2)</code></li> <li>File: <code>l2.c</code></li> </ul>"},{"location":"modules/regularizers/#combined-l1-l2-regularization","title":"Combined L1-L2 Regularization","text":"<ul> <li>Description: Combines L1 and L2 regularization techniques.</li> <li>Function: <code>float l1_l2(float *w, float *dw, float l1, float l2, int n)</code></li> <li>File: <code>l1_l2.c</code></li> </ul>"}]}